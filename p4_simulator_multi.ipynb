{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from keras.models import load_model\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from queue import PriorityQueue\n",
    "\n",
    "# Parameter\n",
    "INPUT_PATH = \"./CSV/\"\n",
    "INPUT_FBFILE_PATH = \"./FB2010-1Hr-150-0.txt\"\n",
    "INPUT_MODEL = \"./Coflow_model_select.h5\"\n",
    "INPUT_MINMAX = \"./min_max.json\"\n",
    "OUTPUT_CSV = \"./P4_RECORD/sampleAndLabel_priority_table_size.csv\"\n",
    "OUTPUT_ACCURACY = \"./P4_RECORD/sampleAndLabel_classify_record.csv\"\n",
    "OUTPUT_COMPLETION_TIME = \"./P4_RECORD/sampleAndLabel_coflow_completion_time.csv\"\n",
    "\n",
    "\n",
    "with open(INPUT_MINMAX) as file_object:\n",
    "    min_max = json.load(file_object)\n",
    "    min_data = np.array(min_max['min_num'])\n",
    "    max_data = np.array(min_max['max_num'])\n",
    "\n",
    "MODEL = load_model(INPUT_MODEL)\n",
    "# COFLOW_NUMBER = 100\n",
    "# FLOW_NUMBER = 10000 \n",
    "CONTROLLER_UPDATE_TIME = 30\n",
    "SKETCH_DEPTH = 3\n",
    "PACKET_CNT_THRESHOLD = 20\n",
    "INITIAL_TTL = 10000\n",
    "INIT_QUEUE_LIMIT = 1048576.0 * 10\n",
    "JOB_SIZE_MULT = 10.0\n",
    "NUM_JOB_QUEUES = 10\n",
    "EGRESS_RATE = 5\n",
    "\n",
    "# Table Size\n",
    "PRIORITY_TABLE_SIZE = 4096 \n",
    "PACKET_CNT_TABLE_SIZE = 512 \n",
    "# PACKET_CNT_TABLE_SIZE = 512 * 2\n",
    "# PACKET_CNT_TABLE_SIZE = 512 * 4\n",
    "# PACKET_CNT_TABLE_SIZE = 512 * 8\n",
    "# PACKET_CNT_TABLE_SIZE = 512 * 16\n",
    "# PACKET_CNT_TABLE_SIZE = 512 * 32\n",
    "# PACKET_CNT_TABLE_SIZE = 512 * 64\n",
    "FLOW_SIZE_TABLE_SIZE = PACKET_CNT_TABLE_SIZE * 4\n",
    "COFLOW_TABLE_SIZE = 10\n",
    "\n",
    "counter = 0\n",
    "# Data Set\n",
    "fb_data = {}\n",
    "fb_coflow_size = {}\n",
    "fb_coflow_priority = {}\n",
    "\n",
    "class Switch:\n",
    "    def __init__(self):\n",
    "        # Queue\n",
    "        self.coflow_queue = {} # Coflow_ID(key), [Flows_List, Real_Coflow_ID] #v\n",
    "        self.input_queue = [] #v\n",
    "        self.output_queue = []\n",
    "        self.wait_queue = PriorityQueue()\n",
    "\n",
    "        # Table\n",
    "        self.priority_table = {} # (Match Table) Flow_ID(key), Priority #v\n",
    "        self.packet_count_table = [[0 for i in range(PACKET_CNT_TABLE_SIZE)] for j in range(SKETCH_DEPTH)] # (Sketch) Packet_Count #v\n",
    "        self.flow_size_table = [[0 for i in range(FLOW_SIZE_TABLE_SIZE)] for j in range(SKETCH_DEPTH)] # (Sketch) Packet_Count #v\n",
    "        self.flow_record_table = {} # (in Controller) Flow_ID(key), Coflow_ID, Priority, Size, TTL, Arrival_Time, Size_m, Finish #v\n",
    "        self.coflow_priority_table = {} # (in Controller) Coflow_ID(key), Coflow_Size, Priority\n",
    "        # Other\n",
    "        self.DNN_counter = 0\n",
    "        self.DNN_right = 0\n",
    "        self.sketch_flow_size = {} #v\n",
    "        self.sketch_cnt_err = 0 #v\n",
    "        self.sketch_size_err = 0 #v\n",
    "        self.sketch_mean_err = 0 #v\n",
    "        self.sketch_counter = 0 #v\n",
    "        self.priority_table_time = []\n",
    "        self.priority_table_size = []\n",
    "        self.packet_collision = [[[] for i in range(PACKET_CNT_TABLE_SIZE)] for j in range(SKETCH_DEPTH)] #v\n",
    "        self.flow_collision = [[[] for i in range(FLOW_SIZE_TABLE_SIZE)] for j in range(SKETCH_DEPTH)] #v\n",
    "        self.pkt_collision_counter = 0 #v\n",
    "        self.flow_collision_counter = 0 #v\n",
    "        self.coflow_completion = {} # Coflow ID(key), Start Time, Completion Time, Duration Time, Coflow Size, Coflow Priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataSet():\n",
    "    global fb_data, fb_coflow_size, fb_coflow_priority\n",
    "    \n",
    "    def getPriority(size):\n",
    "        tmp = INIT_QUEUE_LIMIT\n",
    "        p = 0\n",
    "        while size > tmp:\n",
    "            p += 1\n",
    "            tmp *= JOB_SIZE_MULT\n",
    "            if p >= NUM_JOB_QUEUES:\n",
    "                break\n",
    "        return p\n",
    "\n",
    "    with open(INPUT_FBFILE_PATH, \"r\") as f:\n",
    "        first = True\n",
    "        for line in f:\n",
    "            if first == True:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "            line = line.replace('\\n', '').split(' ')\n",
    "            coflow = float(line[0])\n",
    "            mapper_list = []\n",
    "            reducer_list = []\n",
    "            size_list = []\n",
    "            mapper_num = int(line[2])\n",
    "            for m in range(mapper_num):\n",
    "                mapper_list.append(float(line[3+m]))\n",
    "            reducer_num = int(line[2+int(line[2])+1])\n",
    "            for r in range(reducer_num):\n",
    "                reducer_list.append(float(line[2+int(line[2])+1+r+1].split(':')[0]))\n",
    "                size_list.append(float(line[2+int(line[2])+1+r+1].split(':')[1])) # MB\n",
    "            fb_coflow_size[str(coflow)] = sum(size_list) * 1024 * 1024\n",
    "            fb_coflow_priority[str(coflow)] = getPriority(fb_coflow_size[str(coflow)])\n",
    "            for m in range(mapper_num):\n",
    "                for r in range(reducer_num):\n",
    "                    key = str(coflow) + \"-\" + str(mapper_list[m]) + \"-\" + str(reducer_list[r])\n",
    "                    fb_data[key] = size_list[r] / mapper_num * 1024 ###\n",
    "    # print(fb_data)\n",
    "    # print(fb_coflow_priority)\n",
    "    # print(fb_coflow_size)\n",
    "\n",
    "def loadCsvData():\n",
    "    def sortDir(s):\n",
    "        return int(s.split(\"_\")[0])\n",
    "    input_data = []\n",
    "    input_data_flow = {}\n",
    "    f_cnt = 0\n",
    "    c_list = []\n",
    "    csv_dir = sorted(os.listdir(INPUT_PATH), key=sortDir) # Sort\n",
    "    for f1 in csv_dir: # Packet dir\n",
    "        print(\"open \", f1)\n",
    "        for f2 in sorted(os.listdir(os.path.join(INPUT_PATH, f1))): # Host file\n",
    "            print(f2, end=\" \")\n",
    "            data = np.loadtxt(os.path.join(os.path.join(INPUT_PATH, f1), f2), dtype=float, delimiter=\",\", skiprows=1, usecols=range(8))\n",
    "            for i in range(len(data)): # Packets\n",
    "                c_id = data[i][0] #coflow id\n",
    "                m_id = data[i][3] #mapper id\n",
    "                r_id = data[i][4] #reducer id\n",
    "                key = str(c_id) + \"-\" + str(m_id) + \"-\" + str(r_id)\n",
    "                if c_id not in c_list:\n",
    "                    #if len(c_list) >= COFLOW_NUMBER:\n",
    "                    #    continue\n",
    "                    c_list.append(c_id)\n",
    "                if key not in input_data_flow.keys():\n",
    "                    if key not in fb_data.keys() or data[i][7] == 0:\n",
    "                        continue\n",
    "                    # if f_cnt >= FLOW_NUMBER:\n",
    "                    #     continue\n",
    "                    f_cnt += 1\n",
    "                    input_data_flow[key] = []\n",
    "                input_data_flow[key].append(data[i])\n",
    "            for key in input_data_flow.keys():\n",
    "                num = (fb_data[key] / input_data_flow[key][0][7])\n",
    "                if len(input_data_flow[key]) < num:\n",
    "                    orgin_len = len(input_data_flow[key])\n",
    "                    while len(input_data_flow[key]) < num:\n",
    "                        tmp_input = input_data_flow[key].copy()\n",
    "                        for d in tmp_input:\n",
    "                            if len(input_data_flow[key]) < num:\n",
    "                                input_data_flow[key].append(d)\n",
    "                            else:\n",
    "                                break\n",
    "                    add = 0\n",
    "                    inter = random.sample([1, 2, 3, 4, 6, 7, 8], 1)[0]\n",
    "                    for i in range(len(input_data_flow[key])-orgin_len):\n",
    "                        if (orgin_len + i) % inter == 0:\n",
    "                            add += 1\n",
    "                        input_data_flow[key][orgin_len + i][5] += add\n",
    "        print(\"\")\n",
    "        # if f_cnt >= FLOW_NUMBER:\n",
    "        #     break\n",
    "        # if len(c_list) >= COFLOW_NUMBER:\n",
    "        #    break\n",
    "    for key in input_data_flow.keys():\n",
    "        for d in input_data_flow[key]:\n",
    "            input_data.append(d)\n",
    "    input_data = sorted(input_data, key=lambda s:s[5])\n",
    "    f_id_list = input_data_flow.keys()\n",
    "    return input_data, input_data_flow, f_id_list, c_list\n",
    "\n",
    "def sampling(input_queue, input_data_flow, f_id_list, c_list, k):\n",
    "    shuffle_c_list = sorted(c_list)\n",
    "    random.shuffle(shuffle_c_list)\n",
    "    sample_c_list = shuffle_c_list[:k]\n",
    "    sample_f_id_list = []\n",
    "    for cid in sample_c_list:\n",
    "        for fid in f_id_list:\n",
    "            if(fid.split('-',1)[0] == str(cid)):\n",
    "                sample_f_id_list.append(fid)\n",
    "    set_sample_fid_list = set(sample_f_id_list)\n",
    "    sample_input_data_flow = input_data_flow.copy()\n",
    "    for key in input_data_flow:\n",
    "        if key not in set_sample_fid_list:\n",
    "            sample_input_data_flow.pop(key)\n",
    "    sample_input_queue = []\n",
    "    for item in input_queue:\n",
    "        fid = str(item[0]) + \"-\" + str(item[3]) + \"-\" + str(item[4])\n",
    "        if fid in set_sample_fid_list:\n",
    "            sample_input_queue.append(item)\n",
    "    return sample_input_queue, sample_input_data_flow, sample_f_id_list, sample_c_list\n",
    "\n",
    "def grouping(switches, sample_input_queue, sample_input_flow, sample_f_id_list, k):\n",
    "    shuffle_fid = sample_f_id_list\n",
    "    random.shuffle(shuffle_fid)\n",
    "    shuffle_fid_list = np.array_split(shuffle_fid, k)\n",
    "    shuffle_fid_list_sets = []\n",
    "    for fid_list in shuffle_fid_list:\n",
    "        shuffle_fid_list_sets.append(set(fid_list))\n",
    "    switch_datas = [[]for i in range(k)]\n",
    "    for item in sample_input_queue:\n",
    "        key = str(item[0]) + \"-\" + str(item[3]) + \"-\" + str(item[4])\n",
    "        for fid_list_set in shuffle_fid_list_sets:\n",
    "            if key in fid_list_set:\n",
    "                switch_datas[shuffle_fid_list_sets.index(fid_list_set)].append(item)\n",
    "    for switch in switches:\n",
    "        switch.input_queue = switch_datas[switches.index(switch)]\n",
    "    return switches\n",
    "\n",
    "def grouping2(switches, sample_input_queue, sample_input_flow, sample_f_id_list, sample_c_list, numOfSwitches):\n",
    "    shuffle_cid = sample_c_list\n",
    "    print(\"before shuffle: \",shuffle_cid)\n",
    "    random.shuffle(shuffle_cid)\n",
    "    print(\"after shuffle: \", shuffle_cid)\n",
    "    shuffle_cid_list = np.array_split(shuffle_cid, numOfSwitches)\n",
    "\n",
    "    shuffle_cid_list_sets = []\n",
    "    for cid_list in shuffle_cid_list:\n",
    "        shuffle_cid_list_sets.append(set(cid_list))\n",
    "        \n",
    "    switch_datas = [[]for i in range(numOfSwitches)]\n",
    "    for item in sample_input_queue:\n",
    "        cid = item[0]\n",
    "        for cid_list_set in shuffle_cid_list_sets:\n",
    "            if cid in cid_list_set:\n",
    "                switch_datas[shuffle_cid_list_sets.index(cid_list_set)].append(item)\n",
    "    for switch in switches:\n",
    "        switch.input_queue = switch_datas[switches.index(switch)]\n",
    "    return switches, shuffle_cid_list\n",
    "    \n",
    "def getFlowID(packet, f_id_list):\n",
    "    c_id = packet[0]\n",
    "    m_id = packet[3]\n",
    "    r_id = packet[4]\n",
    "    key = str(c_id) + \"-\" + str(m_id) + \"-\" + str(r_id)\n",
    "    return list(f_id_list).index(key)\n",
    "\n",
    "def checkPriorityTable(switch, f_id, packet):\n",
    "    # priority table\n",
    "    # (Match Table) Flow_ID(key), Priority\n",
    "    find = False\n",
    "    if f_id in switch.priority_table.keys():\n",
    "        packet.append(switch.priority_table[f_id]) # Add priority\n",
    "        find = True\n",
    "    else:\n",
    "        packet.append(0) # Add highest priority\n",
    "    return find, packet\n",
    "\n",
    "def hash(key, width, depth):\n",
    "    h = (key+(depth+1)**(depth)) % width\n",
    "    return h\n",
    "\n",
    "def sketchAction(switch, f_id, table, add_value, clear=False):\n",
    "    global packet_collision, flow_collision, pkt_collision_counter, flow_collision_counter\n",
    "    get_value = []\n",
    "    for i in range(SKETCH_DEPTH):\n",
    "        key = hash(f_id, len(table[i]), i)\n",
    "        table[i][key] += add_value\n",
    "        get_value.append(table[i][key])\n",
    "        if clear:\n",
    "            table[i][key] = 0\n",
    "        # ------ Record ------\n",
    "        if table == switch.packet_count_table: # Add packet count\n",
    "            if add_value != 0:\n",
    "                if f_id not in switch.packet_collision[i][key]:\n",
    "                    if switch.packet_collision[i][key] != []:\n",
    "                        switch.pkt_collision_counter += 1\n",
    "                        print(\"Packet Count Collision - table \", i, \": \", f_id, \" and \", switch.packet_collision[i][key])\n",
    "                    switch.packet_collision[i][key].append(f_id)\n",
    "                    print(\"put fid into packet collision table[\",i,\"][\",key,\"]\")\n",
    "            if clear == True:\n",
    "                # print(packet_collision[i][key])\n",
    "                if f_id in switch.packet_collision[i][key]:\n",
    "                    switch.packet_collision[i][key].remove(f_id)\n",
    "                # print(\"Clear key in packte size: \", f_id)\n",
    "        elif table == switch.flow_size_table: # Add flow size\n",
    "            if add_value != 0:\n",
    "                if f_id not in switch.flow_collision[i][key]:\n",
    "                    if switch.flow_collision[i][key] != []:\n",
    "                        switch.flow_collision_counter += 1\n",
    "                        print(\"Flow Size Collision - table \", i, \": \", f_id, \" and \", switch.flow_collision[i][key])\n",
    "                    switch.flow_collision[i][key].append(f_id) \n",
    "                    print(\"put fid into flow collision table[\",i,\"][\",key,\"]\")\n",
    "            if clear == True:\n",
    "                # print(flow_collision[i][key])\n",
    "                if f_id in switch.flow_collision[i][key]:\n",
    "                    switch.flow_collision[i][key].remove(f_id)\n",
    "                # print(\"Clear key in flow size: \", f_id)\n",
    "        # ------ Record ------\n",
    "    return min(get_value)\n",
    "\n",
    "def updatePacketCntTable(switch, f_id, packet):\n",
    "    cnt = sketchAction(switch, f_id, switch.packet_count_table, 1, False)\n",
    "    if cnt == 1 or cnt == PACKET_CNT_THRESHOLD: \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def updateFlowSizeTable(switch, f_id, packet):\n",
    "    size = sketchAction(switch, f_id, switch.flow_size_table, packet[6], False)\n",
    "    # Record\n",
    "    if f_id not in switch.sketch_flow_size.keys():\n",
    "        switch.sketch_flow_size[f_id] = []\n",
    "    if len(switch.sketch_flow_size[f_id]) < PACKET_CNT_THRESHOLD:\n",
    "        switch.sketch_flow_size[f_id].append(packet[6])\n",
    "    # Record\n",
    "    return size\n",
    "\n",
    "def classify(switch, f_id, packet, packet_m, arrival_t):\n",
    "    def normalize(f_id2, packet_m, arrival_t):\n",
    "        feature_time = abs(arrival_t - switch.flow_record_table[f_id2][4]) / (max_data[0]-min_data[0])\n",
    "        normalize_packet1 = (packet_m - min_data[1]) / (max_data[1] - min_data[1])\n",
    "        normalize_packet2 = (switch.flow_record_table[f_id2][5] - min_data[1]) / (max_data[1] - min_data[1])\n",
    "        return np.array([[feature_time, normalize_packet1, normalize_packet2]])\n",
    "    if len(switch.coflow_queue.keys()) == 0: # Create a new queue\n",
    "        return packet[0] # Real coflow ID\n",
    "    sameScore = []\n",
    "    diffScore = []\n",
    "    sorted_coflow_keys = sorted(switch.coflow_queue.keys())\n",
    "    for i in range(len(sorted_coflow_keys)):\n",
    "        sameScore.append(0)\n",
    "        diffScore.append(0)\n",
    "        cnt = 0\n",
    "        sampleNum = min(len(switch.coflow_queue[sorted_coflow_keys[i]][0]), 20)\n",
    "        sampleList = random.sample(range(len(switch.coflow_queue[sorted_coflow_keys[i]][0])), sampleNum)\n",
    "        for j in sampleList: # Each flow in coflow\n",
    "            if switch.coflow_queue[sorted_coflow_keys[i]][0][j] not in switch.flow_record_table.keys():\n",
    "                continue\n",
    "            n = normalize(switch.coflow_queue[sorted_coflow_keys[i]][0][j], packet_m, arrival_t)\n",
    "            predict_prob = MODEL.predict(n)\n",
    "            predict_classes = predict_prob[0]\n",
    "            sameScore[i] += predict_classes[1]\n",
    "            diffScore[i] += predict_classes[0]\n",
    "            cnt += 1\n",
    "            # ------ Record ------\n",
    "            switch.DNN_counter += 1\n",
    "            if packet[0] == switch.coflow_queue[sorted_coflow_keys[i]][1][j] and predict_classes[1] > predict_classes[0]:\n",
    "                switch.DNN_right += 1\n",
    "            if packet[0] != switch.coflow_queue[sorted_coflow_keys[i]][1][j] and predict_classes[1] <= predict_classes[0]:\n",
    "                switch.DNN_right += 1\n",
    "            # ------ Record ------\n",
    "        if cnt > 0:\n",
    "            sameScore[i] /= cnt\n",
    "            diffScore[i] /= cnt\n",
    "    score = [-1, -1] # [c_id, Max Score]\n",
    "    for i in range(len(sorted_coflow_keys)):\n",
    "        if sameScore[i] > diffScore[i]:\n",
    "            if sameScore[i] > score[1]:\n",
    "                score[1] = sameScore[i]\n",
    "                score[0] = sorted_coflow_keys[i]\n",
    "    if score[1] == -1: # No friend and create a new job\n",
    "        if len(switch.coflow_queue.keys()) < COFLOW_TABLE_SIZE:\n",
    "            c_id = packet[0] # Real coflow ID\n",
    "        else:\n",
    "            # Find the smallest coflow\n",
    "            small = [0, sys.maxsize] # [c_id, #]\n",
    "            for i in range(len(sorted_coflow_keys)):\n",
    "                if len(switch.coflow_queue[sorted_coflow_keys[i]][0]) < small[1]:\n",
    "                    small[0] = sorted_coflow_keys[i]\n",
    "                    small[1] = len(switch.coflow_queue[sorted_coflow_keys[i]][0])\n",
    "            c_id = small[0] # Smallest coflow ID\n",
    "    else:\n",
    "        c_id = score[0] # Existing coflow ID\n",
    "    return c_id\n",
    "\n",
    "#label manually with some error probability\n",
    "def label(packet,c_list,percentage):\n",
    "    prob = random.randrange(0,100)\n",
    "    if prob<percentage:\n",
    "        return packet[0]\n",
    "    else:\n",
    "        cid = random.choice(c_list)\n",
    "        while True:\n",
    "            if cid != packet[0]:\n",
    "                break\n",
    "            else:\n",
    "                cid = random.choice(c_list)\n",
    "        return cid\n",
    "\n",
    "def updateFlowRecordTable(switch, f_id, packet):\n",
    "    #flow_record_table\n",
    "    #                                 0          1        2    3         4         5       6\n",
    "    #(in Controller) Flow_ID(key), Coflow_ID, Priority, Size, TTL, Arrival_Time, Size_m, Finish\n",
    "    # Get data from Packet Table\n",
    "    cnt = sketchAction(switch, f_id, switch.packet_count_table, 0, False)\n",
    "    size = sketchAction(switch, f_id, switch.flow_size_table, 0, False)\n",
    "    if cnt == 1:\n",
    "        print(\"Put \", f_id, \"in Flow Table\")\n",
    "        if f_id in switch.flow_record_table.keys():\n",
    "            print(\"(cnt = 1) Flow \", f_id, \" is in Flow Table\")\n",
    "            switch.flow_record_table[f_id][6] = False #finish = false\n",
    "            switch.flow_record_table[f_id][3] = INITIAL_TTL\n",
    "        else:\n",
    "            switch.flow_record_table[f_id] = [None, 0, size, INITIAL_TTL, packet[5], 0, False]\n",
    "        return\n",
    "    elif cnt == PACKET_CNT_THRESHOLD:\n",
    "        sketchAction(switch, f_id, switch.packet_count_table, 0, True) # Reset\n",
    "        # Classify\n",
    "        packet_m = size / cnt\n",
    "        # ------ Record ------\n",
    "        if f_id in switch.sketch_flow_size.keys(): \n",
    "            real_packet_s = sum(switch.sketch_flow_size[f_id])\n",
    "            real_packet_c = len(switch.sketch_flow_size[f_id])\n",
    "            real_packet_m = real_packet_s/real_packet_c\n",
    "            if math.isnan(abs(real_packet_s - size) / real_packet_s) == False and math.isnan(abs(real_packet_c - cnt) / real_packet_c) == False and math.isnan(abs(real_packet_m - packet_m) / (real_packet_s/real_packet_c)) == False:\n",
    "                switch.sketch_size_err += abs(real_packet_s - size) / real_packet_s\n",
    "                switch.sketch_cnt_err += abs(real_packet_c - cnt) / real_packet_c\n",
    "                switch.sketch_mean_err += abs(real_packet_m - packet_m) / (real_packet_s/real_packet_c)\n",
    "                switch.sketch_counter += 1\n",
    "                print(\"------ \", f_id, \" ------\")\n",
    "                print(\"sketch size: \", size, \" real size: \", real_packet_s)\n",
    "                print(\"sketch cnt: \", cnt, \" real cnt: \", real_packet_c)\n",
    "                print(\"sketch mean: \", packet_m, \" real mean: \", real_packet_s/real_packet_c)\n",
    "                print(\"-----------------\")\n",
    "        # ------ Record ------\n",
    "        if f_id not in switch.flow_record_table.keys():\n",
    "            print(\"(cnt = \", PACKET_CNT_THRESHOLD, \") Flow \", f_id, \" is not in Flow Table\")\n",
    "            switch.flow_record_table[f_id] = [None, 0, size, INITIAL_TTL, packet[5], packet_m, False]\n",
    "        else:\n",
    "            switch.flow_record_table[f_id][2] = size\n",
    "            switch.flow_record_table[f_id][5] = packet_m\n",
    "        arrival_t = switch.flow_record_table[f_id][4]\n",
    "        #real classify\n",
    "        c_id = classify(switch, f_id, packet, packet_m, arrival_t)\n",
    "        #label manually with some error probability\n",
    "        #c_id = label(packet, c_list, 80)\n",
    "        print(c_id)\n",
    "        # ------ Record ------\n",
    "        real_coflow_id = packet[0]\n",
    "        real_size = fb_coflow_size[str(real_coflow_id)]\n",
    "        classified_size = fb_coflow_size[str(float(c_id))]\n",
    "        real_priority = fb_coflow_priority[str(real_coflow_id)]\n",
    "        classified_priority = fb_coflow_priority[str(float(c_id))]\n",
    "        with open(OUTPUT_ACCURACY, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([f_id, real_coflow_id, c_id, real_size, classified_size, real_priority, classified_priority])\n",
    "        # ------ Record ------\n",
    "        # Update Coflow Data and set priority\n",
    "        priority = 0\n",
    "        if c_id in switch.coflow_queue.keys(): # Record in Coflow Queue for classify\n",
    "            switch.coflow_queue[c_id][0].append(f_id)\n",
    "            switch.coflow_queue[c_id][1].append(packet[0]) # Real coflow id of this flow \n",
    "        else:\n",
    "            switch.coflow_queue[c_id] = [[f_id],[packet[0]]]\n",
    "        if c_id in switch.coflow_priority_table.keys(): # Update priority\n",
    "            priority = switch.coflow_priority_table[c_id][1]\n",
    "        else: # New coflow\n",
    "            switch.coflow_priority_table[c_id] = [0, 0]\n",
    "        # Update Flow Table\n",
    "        switch.flow_record_table[f_id][0] = c_id\n",
    "        switch.flow_record_table[f_id][1] = priority\n",
    "        # Insert to Priority Table\n",
    "        if len(switch.priority_table) < PRIORITY_TABLE_SIZE:\n",
    "            switch.priority_table[f_id] = priority\n",
    "        else:\n",
    "            print(\"(Priority Table) Overflow\")\n",
    "            # Todo\n",
    "\n",
    "def schedule(table):\n",
    "    for c_id in table.keys():\n",
    "        size = table[c_id][0] * 1024 ##\n",
    "        curPriority = 0\n",
    "        tmp = INIT_QUEUE_LIMIT\n",
    "        while size > tmp:\n",
    "            curPriority += 1\n",
    "            tmp *= JOB_SIZE_MULT\n",
    "            if curPriority >= NUM_JOB_QUEUES:\n",
    "                break\n",
    "        table[c_id][1] = curPriority\n",
    "    return table\n",
    "\n",
    "def controllerUpdate(switch):\n",
    "    # Update Flow Table (Size)\n",
    "    coflow_size = {} # Coflow id, coflow size \n",
    "    for f_id in switch.flow_record_table.keys():\n",
    "        if switch.flow_record_table[f_id][6] == False: #if flow not finished\n",
    "            size = sketchAction(switch, f_id, switch.flow_size_table, 0, False)\n",
    "            switch.flow_record_table[f_id][2] = size #update flow size\n",
    "        # For next step\n",
    "        if switch.flow_record_table[f_id][0] != None: #if flow has coflow id\n",
    "            if switch.flow_record_table[f_id][0] not in coflow_size.keys():\n",
    "                coflow_size[switch.flow_record_table[f_id][0]] = switch.flow_record_table[f_id][2]\n",
    "            else:\n",
    "                coflow_size[switch.flow_record_table[f_id][0]] += switch.flow_record_table[f_id][2]  \n",
    "    # Update coflow size\n",
    "    for c_id in switch.coflow_priority_table.keys():\n",
    "        if c_id not in coflow_size.keys(): # Bug\n",
    "            continue\n",
    "        switch.coflow_priority_table[c_id][0] = coflow_size[c_id] \n",
    "    # Schedule\n",
    "    switch.coflow_priority_table = schedule(switch.coflow_priority_table) # Update coflow priority\n",
    "    # print(\"Coflow Table\", coflow_priority_table)\n",
    "    # Update Flow Table (Priority)\n",
    "    update_flow_list = [] # Flow ID, Priority\n",
    "    for f_id in switch.flow_record_table.keys():\n",
    "        if switch.flow_record_table[f_id][0] != None: # Classified\n",
    "            if switch.flow_record_table[f_id][1] != switch.coflow_priority_table[switch.flow_record_table[f_id][0]][1]: # Update priority\n",
    "                switch.flow_record_table[f_id][1] = switch.coflow_priority_table[switch.flow_record_table[f_id][0]][1]\n",
    "                update_flow_list.append([f_id, switch.flow_record_table[f_id][1]])\n",
    "    # Update Priority Table\n",
    "    for entry in update_flow_list:\n",
    "        if entry[0] not in switch.priority_table.keys():\n",
    "            print(\"(Update priority in Priority Table) Flow \", f_id, \" is not in Priority Table\")\n",
    "            if len(switch.priority_table) < PRIORITY_TABLE_SIZE:\n",
    "                switch.priority_table[f_id] = entry[1]\n",
    "            else:\n",
    "                print(\"(Priority Table) Overflow\")\n",
    "                # Todo\n",
    "        else:\n",
    "            switch.priority_table[entry[0]] = entry[1]\n",
    "    return switch.coflow_priority_table\n",
    "\n",
    "def controllerUpdateTTL(switch, f_id):\n",
    "    clear_now = []\n",
    "    finished_coflow = {}\n",
    "    # Update TTL\n",
    "    for f in switch.flow_record_table.keys(): \n",
    "        if f == f_id:\n",
    "            switch.flow_record_table[f_id][3] = INITIAL_TTL\n",
    "            switch.flow_record_table[f][6] = False\n",
    "        else:\n",
    "            switch.flow_record_table[f][3] -= 1\n",
    "            if switch.flow_record_table[f][3] <= 0 and switch.flow_record_table[f][6] == False:\n",
    "                print(counter, \" ############### Clear\", f_id)\n",
    "                if switch.flow_record_table[f][0] == None: \n",
    "                    sketchAction(switch, f, switch.packet_count_table, 0, True)\n",
    "                    clear_now.append(f)\n",
    "                else: # Classified\n",
    "                    if f in switch.priority_table.keys():\n",
    "                        del switch.priority_table[f]\n",
    "                        sketchAction(switch, f, switch.flow_size_table, 0, True)\n",
    "                    switch.flow_record_table[f][6] = True\n",
    "        if switch.flow_record_table[f][0] != None:\n",
    "            if switch.flow_record_table[f][0] not in finished_coflow.keys():\n",
    "                finished_coflow[switch.flow_record_table[f][0]] = True\n",
    "            if switch.flow_record_table[f][6] == False: # Flow unfinished\n",
    "                finished_coflow[switch.flow_record_table[f][0]] = False # Coflow unfinished\n",
    "    # Delete finished coflows\n",
    "    for c_id in finished_coflow.keys(): \n",
    "        if finished_coflow[c_id] == True:\n",
    "            del switch.coflow_priority_table[c_id]\n",
    "            for f in set(switch.coflow_queue[c_id][0]):\n",
    "                if f in switch.flow_record_table.keys():\n",
    "                    del switch.flow_record_table[f]\n",
    "            del switch.coflow_queue[c_id]\n",
    "    # Delete finished flows       \n",
    "    for f in clear_now:\n",
    "        del switch.flow_record_table[f]\n",
    "    return\n",
    "    \n",
    "def PIFO(packet, switch):\n",
    "    #print(\"PIFO -> packet = \", packet)\n",
    "    #print(\"PIFO -> packet type : \", type(packet))\n",
    "    switch.wait_queue.put((packet[-1],packet))\n",
    "    #print(\"PIFO -> after put : \", wait_queue.get())\n",
    "    return switch.wait_queue\n",
    "\n",
    "def egress(switch):\n",
    "    item = switch.wait_queue.get()\n",
    "    out_packet = item[1]\n",
    "    switch.output_queue.append(out_packet)\n",
    "    # ------ Record ------\n",
    "    if out_packet[0] not in switch.coflow_completion.keys():\n",
    "        switch.coflow_completion[out_packet[0]] = [counter, counter, 0, fb_coflow_size[str(out_packet[0])], fb_coflow_priority[str(out_packet[0])]]\n",
    "    else:\n",
    "        switch.coflow_completion[out_packet[0]][1] = counter\n",
    "        switch.coflow_completion[out_packet[0]][2] = counter - switch.coflow_completion[out_packet[0]][0]\n",
    "    # ------ Record ------\n",
    "    return switch.output_queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readDataSet()\n",
    "print(\"Read packets data: \")\n",
    "input_queue, input_data_flow, f_id_list, c_list = loadCsvData()\n",
    "print(len(c_list), \" coflows, \", len(f_id_list), \" flows and \", len(input_queue), \" packets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sampling: \n",
      "10  coflows,  615  flows and  54813  packets\n"
     ]
    }
   ],
   "source": [
    "#sampling\n",
    "sample_limit = 100000\n",
    "sample_input_queue, sample_input_data_flow, sample_f_id_list, sample_c_list = sampling(input_queue, input_data_flow, f_id_list, c_list, 10)\n",
    "while len(sample_input_queue)>sample_limit:\n",
    "    sample_input_queue, sample_input_data_flow, sample_f_id_list, sample_c_list = sampling(input_queue, input_data_flow, f_id_list, c_list, 10)\n",
    "print(\"After sampling: \")\n",
    "print(len(sample_c_list), \" coflows, \", len(sample_f_id_list), \" flows and \", len(sample_input_queue), \" packets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before shuffle:  [358.0, 50.0, 287.0, 87.0, 425.0, 330.0, 338.0, 351.0, 493.0, 469.0]\n",
      "after shuffle:  [87.0, 469.0, 351.0, 287.0, 358.0, 425.0, 493.0, 338.0, 330.0, 50.0]\n",
      "25597\n",
      "29216\n"
     ]
    }
   ],
   "source": [
    "#grouping\n",
    "switches=[]\n",
    "numOfSwitches = 2\n",
    "for i in range(numOfSwitches):\n",
    "    switches.append(Switch())\n",
    "#switches = grouping(switches, sample_input_queue, sample_input_data_flow, sample_f_id_list, numOfSwitches)\n",
    "switches, shuffle_cid_list = grouping2(switches, sample_input_queue, sample_input_data_flow, sample_f_id_list, sample_c_list, numOfSwitches)\n",
    "for switch in switches:\n",
    "    print(len(switch.input_queue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'87.0': 1964, '287.0': 6219, '351.0': 7934, '358.0': 8320, '469.0': 1155}\n",
      "{'50.0': 2733, '330.0': 5490, '338.0': 17747, '425.0': 1614, '493.0': 1627}\n"
     ]
    }
   ],
   "source": [
    "cid_parition = []\n",
    "for switch in switches:\n",
    "    cid = {}\n",
    "    for item in switch.input_queue:\n",
    "        if str(item[0]) not in cid.keys():\n",
    "            cid[str(item[0])]=0\n",
    "        else:\n",
    "            cid[str(item[0])]+=1\n",
    "    cid_parition.append(cid)\n",
    "for cid_list in cid_parition:\n",
    "    print(cid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "packet_index = -1\n",
    "while True:\n",
    "    counter += 1 # timer\n",
    "    packet_index += 1\n",
    "    if packet_index < len(switches[0].input_queue):\n",
    "        this_packet = list(switches[0].input_queue[packet_index])\n",
    "        f_id = getFlowID(this_packet, f_id_list)\n",
    "        # Add priority into packet header\n",
    "        find, this_packet = checkPriorityTable(switches[0], f_id, this_packet)\n",
    "        if not find:\n",
    "            # Update Packet Count Table\n",
    "            action = updatePacketCntTable(switches[0], f_id, this_packet)\n",
    "        # Update Flow Size Table\n",
    "        updateFlowSizeTable(switches[0], f_id, this_packet)\n",
    "        # New flow or Packet full, inform controller\n",
    "        if not find and action:\n",
    "            updateFlowRecordTable(switches[0], f_id, this_packet)\n",
    "        # Controller update\n",
    "        if counter % CONTROLLER_UPDATE_TIME == 0 or packet_index == len(switches[0].input_queue)-1:\n",
    "            switches[0].coflow_priority_table = controllerUpdate(switches[0]) \n",
    "        # PIFO\n",
    "        switches[0].wait_queue = PIFO(this_packet, switches[0])\n",
    "        \n",
    "        \n",
    "    # Egress\n",
    "    if counter % EGRESS_RATE == 0:\n",
    "        switches[0].output_queue = egress(switches[0])\n",
    "    # Print Result\n",
    "    if counter % 100 == 0:\n",
    "        print(\"Time slot: \", counter)\n",
    "        print(\"Size of Priority Table: \", len(switches[0].priority_table.keys()))\n",
    "        switches[0].priority_table_time.append(counter)\n",
    "        switches[0].priority_table_size.append(len(switches[0].priority_table.keys()))\n",
    "        if switches[0].DNN_counter != 0:\n",
    "            print(\"DNN Accuracy: \", switches[0].DNN_right / switches[0].DNN_counter * 100, \" %\")\n",
    "        if switches[0].sketch_counter != 0:\n",
    "            print(\"Sketch Count Err: \", switches[0].sketch_cnt_err / switches[0].sketch_counter * 100, \" %\")\n",
    "            print(\"Sketch Size Err: \", switches[0].sketch_size_err / switches[0].sketch_counter * 100, \" %\")\n",
    "            print(\"Sketch Mean Err: \", switches[0].sketch_mean_err / switches[0].sketch_counter * 100, \" %\")\n",
    "        print(\"len of wait queue: \", len(switches[0].wait_queue.queue))\n",
    "    # Update TTL\n",
    "    controllerUpdateTTL(switches[0], f_id)\n",
    "    # Completed\n",
    "    if counter >= len(switches[0].input_queue) and len(switches[0].wait_queue.queue) == 0: # stop\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Completed\n",
      "Write Completed\n"
     ]
    }
   ],
   "source": [
    "# ------ Record ------\n",
    "for switch in switches:\n",
    "    with open(OUTPUT_CSV+str(switches.index(switch)), \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\")\n",
    "        writer.writerow([\"Time slot\", \"Size\"])\n",
    "        for i in range(len(switch.priority_table_time)):\n",
    "            writer.writerow([switch.priority_table_time[i], switch.priority_table_size[i]])\n",
    "        print(\"Write Completed\")\n",
    "    with open(OUTPUT_COMPLETION_TIME+str(switches.index(switch)), \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Coflow ID\", \"Start Time\", \"Completion Time\", \"Duration Time\", \"Coflow Size\", \"Coflow Priority\"])\n",
    "        for k, v in switch.coflow_completion.items():\n",
    "            tmp = [k]\n",
    "            tmp.extend(v)\n",
    "            writer.writerow(tmp)\n",
    "# ------ Record ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_COMPLETION_TIME+str(1), \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Coflow ID\", \"Start Time\", \"Completion Time\", \"Duration Time\", \"Coflow Size\", \"Coflow Priority\"])\n",
    "    for k, v in switches[1].coflow_completion.items():\n",
    "        tmp = [k]\n",
    "        tmp.extend(v)\n",
    "        writer.writerow(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{87.0: [5, 83455, 83450, 14680064.0, 1], 287.0: [1970, 107030, 105060, 851443712.0, 2], 351.0: [8190, 114445, 106255, 6656360448.0, 3], 358.0: [37700, 127800, 90100, 115343360.0, 2], 469.0: [46135, 127985, 81850, 2097152.0, 0]}\n",
      "{50.0: [5, 51370, 51365, 377487360.0, 2], 330.0: [5290, 67295, 62005, 28311552.0, 1], 338.0: [7640, 146080, 138440, 10846470144.0, 4], 425.0: [25975, 40350, 14375, 41943040.0, 1], 493.0: [27590, 46780, 19190, 198180864.0, 2]}\n",
      "140315785169312\n",
      "140315785168880\n",
      "{469.0: [[556075], [469.0]]}\n",
      "{493.0: [[613399, 613400, 613401], [493.0, 493.0, 493.0]]}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "class Switch:\n",
    "    # Queue\n",
    "    coflow_queue = {} # Coflow_ID(key), [Flows_List, Real_Coflow_ID] #v\n",
    "    input_queue = [] #v\n",
    "    output_queue = []\n",
    "    wait_queue = PriorityQueue()\n",
    "\n",
    "    # Table\n",
    "    priority_table = {} # (Match Table) Flow_ID(key), Priority #v\n",
    "    packet_count_table = [[0 for i in range(PACKET_CNT_TABLE_SIZE)] for j in range(SKETCH_DEPTH)] # (Sketch) Packet_Count #v\n",
    "    flow_size_table = [[0 for i in range(FLOW_SIZE_TABLE_SIZE)] for j in range(SKETCH_DEPTH)] # (Sketch) Packet_Count #v\n",
    "    flow_record_table = {} # (in Controller) Flow_ID(key), Coflow_ID, Priority, Size, TTL, Arrival_Time, Size_m, Finish #v\n",
    "    coflow_priority_table = {} # (in Controller) Coflow_ID(key), Coflow_Size, Priority\n",
    "    # Other\n",
    "    DNN_counter = 0\n",
    "    DNN_right = 0\n",
    "    sketch_flow_size = {} #v\n",
    "    sketch_cnt_err = 0 #v\n",
    "    sketch_size_err = 0 #v\n",
    "    sketch_mean_err = 0 #v\n",
    "    sketch_counter = 0 #v\n",
    "    priority_table_time = []\n",
    "    priority_table_size = []\n",
    "    packet_collision = [[[] for i in range(PACKET_CNT_TABLE_SIZE)] for j in range(SKETCH_DEPTH)] #v\n",
    "    flow_collision = [[[] for i in range(FLOW_SIZE_TABLE_SIZE)] for j in range(SKETCH_DEPTH)] #v\n",
    "    pkt_collision_counter = 0 #v\n",
    "    flow_collision_counter = 0 #v\n",
    "    coflow_completion = {} # Coflow ID(key), Start Time, Completion Time, Duration Time, Coflow Size, Coflow Priority\n",
    "'''\n",
    "print(switches[0].coflow_completion)\n",
    "print(switches[1].coflow_completion)\n",
    "print(id(switches[0].wait_queue))\n",
    "print(id(switches[1].wait_queue))\n",
    "print(switches[0].coflow_queue)\n",
    "print(switches[1].coflow_queue)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6deef0af540e643d71b80c868ee36b0d137a9cc523e62c0aa4690283770f54f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
